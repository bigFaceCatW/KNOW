#============== kafka ===================
# 指定kafka 代理地址，可以多个
spring.kafka.bootstrap-servers=10.190.3.172:5889,10.190.5.208:5889,10.190.5.209:5889
spring.kafka.template.default-topic=factCat; 
#===============密码鉴权配置,如果配置了密码=====
#spring.kafka.properties.security.protocol=SASL_PLAINTEXT
#spring.kafka.properties.sasl.mechanism=SCRAM-SHA-256
#spring.kafka.properties.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="kc-kafka-cluster-sui" password="i4W#N@BA";
#=============== provider  =======================
#发生错误后，消息重发的次数
spring.kafka.producer.retries=2
#kafka.producer.sasl-jaas-config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\"producer\" password=\"123456\";
# 每次批量发送消息的数量
#spring.kafka.producer.batch-size=16384
#spring.kafka.producer.buffer-memory=33554432

# 指定消息key和消息体的编解码方式
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.IntegerSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#=============== consumer  =======================
# 指定消息组，带上groupid，topic的消息会分发到10个consumer上，每条消息只被消费1次
spring.kafka.consumer.group-id=group1
#指定新的consumer没消费会从最近地方开始消费latest
#消费者将从最新的记录开始读取数据
spring.kafka.consumer.auto-offset-reset=earliest
#默认指定消息被消费之后自动提交偏移量，以便下次继续消费，不提交就会重复消费之前的
spring.kafka.consumer.enable-auto-commit=true
spring.kafka.consumer.auto-commit-interval=1000

# 指定消息key和消息体的编解码方式
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.IntegerDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#在侦听器容器中运行的线程数
#spring.kafka.listener.concurrency=5
#spring.kafka.listener.ack-mode=manual_immediate